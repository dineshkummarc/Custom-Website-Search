<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><html dir="LTR"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8" /><meta content="history" name="save" /><meta name="vs_targetSchema" content="http://schemas.microsoft.com/intellisense/ie5" /><title>Lucene.Net.Analysis</title><xml></xml><link rel="stylesheet" type="text/css" href="MSDN.css" /></head><body id="bodyID" class="dtBODY"><script type="text/javascript">
    	window.onload = function() {
	    	var i = window.frames["iframe_navi"];
	    	i.syncTree(document.URL);
    	}
    </script><div id="navi"><iframe src="contents.html" id="iframe_navi" name="iframe_navi"></iframe></div><div id="nsbanner"><div id="bannerrow1"><table class="bannerparthead" cellspacing="0"><tr id="hdr"><td class="runninghead">Lucene.Net</td><td class="product"></td></tr></table></div><div id="TitleRow"><h1 class="dtH1">Lucene.Net.Analysis Namespace</h1></div></div><div id="nstext"><p><a href="Lucene.Net.Analysis~Hierarchy.html">Namespace Hierarchy</a></p><h3 class="dtH3">Classes</h3><div class="tablediv"><table class="dtTABLE" cellspacing="0"><tr valign="top"><th width="50%">Class</th><th width="50%">Description</th></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.Analyzer.html">Analyzer</a></td><td width="50%">An Analyzer builds TokenStreams, which analyze text. It thus represents a policy for extracting index terms from text. <p xmlns="urn:ndoc-schema"></p> Typical implementations first build a Tokenizer, which breaks the stream of characters from the Reader into raw Tokens. One or more TokenFilters may then be applied to the output of the Tokenizer. </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.ASCIIFoldingFilter.html">ASCIIFoldingFilter</a></td><td width="50%"> This class converts alphabetic, numeric, and symbolic Unicode characters which are not in the first 127 ASCII characters (the "Basic Latin" Unicode block) into their ASCII equivalents, if one exists. Characters from the following Unicode blocks are converted; however, only those characters with reasonable ASCII alternatives are converted: <ul type="disc" xmlns:ndoc="urn:ndoc-schema"><li>C1 Controls and Latin-1 Supplement: <a href="http://www.unicode.org/charts/PDF/U0080.pdf" xmlns="urn:ndoc-schema">http://www.unicode.org/charts/PDF/U0080.pdf</a></li><li>Latin Extended-A: <a href="http://www.unicode.org/charts/PDF/U0100.pdf" xmlns="urn:ndoc-schema">http://www.unicode.org/charts/PDF/U0100.pdf</a></li><li>Latin Extended-B: <a href="http://www.unicode.org/charts/PDF/U0180.pdf" xmlns="urn:ndoc-schema">http://www.unicode.org/charts/PDF/U0180.pdf</a></li><li>Latin Extended Additional: <a href="http://www.unicode.org/charts/PDF/U1E00.pdf" xmlns="urn:ndoc-schema">http://www.unicode.org/charts/PDF/U1E00.pdf</a></li><li>Latin Extended-C: <a href="http://www.unicode.org/charts/PDF/U2C60.pdf" xmlns="urn:ndoc-schema">http://www.unicode.org/charts/PDF/U2C60.pdf</a></li><li>Latin Extended-D: <a href="http://www.unicode.org/charts/PDF/UA720.pdf" xmlns="urn:ndoc-schema">http://www.unicode.org/charts/PDF/UA720.pdf</a></li><li>IPA Extensions: <a href="http://www.unicode.org/charts/PDF/U0250.pdf" xmlns="urn:ndoc-schema">http://www.unicode.org/charts/PDF/U0250.pdf</a></li><li>Phonetic Extensions: <a href="http://www.unicode.org/charts/PDF/U1D00.pdf" xmlns="urn:ndoc-schema">http://www.unicode.org/charts/PDF/U1D00.pdf</a></li><li>Phonetic Extensions Supplement: <a href="http://www.unicode.org/charts/PDF/U1D80.pdf" xmlns="urn:ndoc-schema">http://www.unicode.org/charts/PDF/U1D80.pdf</a></li><li>General Punctuation: <a href="http://www.unicode.org/charts/PDF/U2000.pdf" xmlns="urn:ndoc-schema">http://www.unicode.org/charts/PDF/U2000.pdf</a></li><li>Superscripts and Subscripts: <a href="http://www.unicode.org/charts/PDF/U2070.pdf" xmlns="urn:ndoc-schema">http://www.unicode.org/charts/PDF/U2070.pdf</a></li><li>Enclosed Alphanumerics: <a href="http://www.unicode.org/charts/PDF/U2460.pdf" xmlns="urn:ndoc-schema">http://www.unicode.org/charts/PDF/U2460.pdf</a></li><li>Dingbats: <a href="http://www.unicode.org/charts/PDF/U2700.pdf" xmlns="urn:ndoc-schema">http://www.unicode.org/charts/PDF/U2700.pdf</a></li><li>Supplemental Punctuation: <a href="http://www.unicode.org/charts/PDF/U2E00.pdf" xmlns="urn:ndoc-schema">http://www.unicode.org/charts/PDF/U2E00.pdf</a></li><li>Alphabetic Presentation Forms: <a href="http://www.unicode.org/charts/PDF/UFB00.pdf" xmlns="urn:ndoc-schema">http://www.unicode.org/charts/PDF/UFB00.pdf</a></li><li>Halfwidth and Fullwidth Forms: <a href="http://www.unicode.org/charts/PDF/UFF00.pdf" xmlns="urn:ndoc-schema">http://www.unicode.org/charts/PDF/UFF00.pdf</a></li></ul> See: <a href="http://en.wikipedia.org/wiki/Latin_characters_in_Unicode" xmlns="urn:ndoc-schema">http://en.wikipedia.org/wiki/Latin_characters_in_Unicode</a> The set of character conversions supported by this class is a superset of those supported by Lucene's <a href="Lucene.Net.Analysis.ISOLatin1AccentFilter.html">ISOLatin1AccentFilter</a> which strips accents from Latin1 characters. For example, 'À' will be replaced by 'a'. </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.BaseCharFilter.html">BaseCharFilter</a></td><td width="50%"> * Base utility class for implementing a <a href="Lucene.Net.Analysis.CharFilter.html">CharFilter</a>. * You subclass this, and then record mappings by calling * <a href="Lucene.Net.Analysis.BaseCharFilter.AddOffCorrectMap.html">AddOffCorrectMap</a>, and then invoke the correct * method to correct an offset. </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.CachingTokenFilter.html">CachingTokenFilter</a></td><td width="50%"> This class can be used if the token attributes of a TokenStream are intended to be consumed more than once. It caches all token attribute states locally in a List. <p xmlns="urn:ndoc-schema"></p>CachingTokenFilter implements the optional method <a href="Lucene.Net.Analysis.TokenStream.Reset.html">Reset</a>, which repositions the stream to the first Token. </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.CharArraySet.html">CharArraySet</a></td><td width="50%"> A simple class that stores Strings as char[]'s in a hash table. Note that this is not a general purpose class. For example, it cannot remove items from the set, nor does it resize its hash table to be smaller, etc. It is designed to be quick to test if a char[] is in the set without the necessity of converting it to a String first. <p xmlns="urn:ndoc-schema"></p><em xmlns="urn:ndoc-schema">Please note:</em> This class implements <a href="http://msdn.microsoft.com/en-us/library/System.Collections.Generic.ISet`1(VS.100).aspx">ISet`1</a> but does not behave like it should in all cases. The generic type is <a href="http://msdn.microsoft.com/en-us/library/System.Collections.Generic.ICollection`1(VS.100).aspx">ICollection`1</a>, because you can add any object to it, that has a string representation. The add methods will use <a href="http://msdn.microsoft.com/en-us/library/System.Object.ToString(VS.100).aspx">ToString</a> and store the result using a <a href="http://msdn.microsoft.com/en-us/library/System.Char(VS.100).aspx">Char</a> buffer. The same behaviour have the <a href="Lucene.Net.Analysis.CharArraySet.Contains3.html">Contains</a> methods. The <a href="Lucene.Net.Analysis.CharArraySet.GetEnumerator.html">GetEnumerator</a> method returns an <a href="http://msdn.microsoft.com/en-us/library/System.String(VS.100).aspx">String</a> IEnumerable. For type safety also {@link #stringIterator()} is provided. </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.CharArraySet.CharArraySetEnumerator.html">CharArraySet.CharArraySetEnumerator</a></td><td width="50%"> The IEnumerator&lt;String&gt; for this set. Strings are constructed on the fly, so use <code xmlns:ndoc="urn:ndoc-schema">nextCharArray</code> for more efficient access </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.CharFilter.html">CharFilter</a></td><td width="50%"> Subclasses of CharFilter can be chained to filter CharStream. They can be used as <a href="http://msdn.microsoft.com/en-us/library/System.IO.TextReader(VS.100).aspx">TextReader</a> with additional offset correction. <a href="Lucene.Net.Analysis.Tokenizer.html">Tokenizer</a>s will automatically use <a href="Lucene.Net.Analysis.CharFilter.CorrectOffset.html">CorrectOffset</a> if a CharFilter/CharStream subclass is used. </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.CharReader.html">CharReader</a></td><td width="50%"> CharReader is a Reader wrapper. It reads chars from Reader and outputs <a href="Lucene.Net.Analysis.CharStream.html">CharStream</a>, defining an identify function <a href="Lucene.Net.Analysis.CharReader.CorrectOffset.html">CorrectOffset</a> method that simply returns the provided offset. </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.CharStream.html">CharStream</a></td><td width="50%"> CharStream adds <a href="Lucene.Net.Analysis.CharStream.CorrectOffset.html">CorrectOffset</a> functionality over <a href="http://msdn.microsoft.com/en-us/library/System.IO.TextReader(VS.100).aspx">TextReader</a>. All Tokenizers accept a CharStream instead of <b xmlns:ndoc="urn:ndoc-schema">TextReader</b> as input, which enables arbitrary character based filtering before tokenization. The <b xmlns:ndoc="urn:ndoc-schema">CorrectOffset</b> method fixed offsets to account for removal or insertion of characters, so that the offsets reported in the tokens match the character offsets of the original Reader. </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.CharTokenizer.html">CharTokenizer</a></td><td width="50%">An abstract base class for simple, character-oriented tokenizers.</td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.ISOLatin1AccentFilter.html">ISOLatin1AccentFilter</a></td><td width="50%"><FONT color="red"><B>Obsolete. </B></FONT> A filter that replaces accented characters in the ISO Latin 1 character set (ISO-8859-1) by their unaccented equivalent. The case will not be altered. <p xmlns="urn:ndoc-schema"></p> For instance, 'À' will be replaced by 'a'. <p xmlns="urn:ndoc-schema"></p></td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.KeywordAnalyzer.html">KeywordAnalyzer</a></td><td width="50%"> "Tokenizes" the entire stream as a single token. This is useful for data like zip codes, ids, and some product names. </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.KeywordTokenizer.html">KeywordTokenizer</a></td><td width="50%"> Emits the entire input as a single token.</td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.LengthFilter.html">LengthFilter</a></td><td width="50%">Removes words that are too long or too short from the stream.</td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.LetterTokenizer.html">LetterTokenizer</a></td><td width="50%">A LetterTokenizer is a tokenizer that divides text at non-letters. That's to say, it defines tokens as maximal strings of adjacent letters, as defined by java.lang.Character.isLetter() predicate. Note: this does a decent job for most European languages, but does a terrible job for some Asian languages, where words are not separated by spaces. </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.LowerCaseFilter.html">LowerCaseFilter</a></td><td width="50%">Normalizes token text to lower case.</td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.LowerCaseTokenizer.html">LowerCaseTokenizer</a></td><td width="50%"> LowerCaseTokenizer performs the function of LetterTokenizer and LowerCaseFilter together. It divides text at non-letters and converts them to lower case. While it is functionally equivalent to the combination of LetterTokenizer and LowerCaseFilter, there is a performance advantage to doing the two tasks at once, hence this (redundant) implementation. <p xmlns="urn:ndoc-schema"></p> Note: this does a decent job for most European languages, but does a terrible job for some Asian languages, where words are not separated by spaces. </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.MappingCharFilter.html">MappingCharFilter</a></td><td width="50%"> Simplistic <a href="Lucene.Net.Analysis.CharFilter.html">CharFilter</a> that applies the mappings contained in a <a href="Lucene.Net.Analysis.NormalizeCharMap.html">NormalizeCharMap</a> to the character stream, and correcting the resulting changes to the offsets. </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.NormalizeCharMap.html">NormalizeCharMap</a></td><td width="50%"> Holds a map of String input to String output, to be used with <a href="Lucene.Net.Analysis.MappingCharFilter.html">MappingCharFilter</a>. </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.NumericTokenStream.html">NumericTokenStream</a></td><td width="50%"><b xmlns="urn:ndoc-schema">Expert:</b> This class provides a <a href="Lucene.Net.Analysis.TokenStream.html">TokenStream</a> for indexing numeric values that can be used by <a href="Lucene.Net.Search.NumericRangeQuery$1.html">NumericRangeQuery(T)</a> or <a href="Lucene.Net.Search.NumericRangeFilter$1.html">NumericRangeFilter(T)</a>. <p xmlns="urn:ndoc-schema"></p>Note that for simple usage, <a href="Lucene.Net.Documents.NumericField.html">NumericField</a> is recommended. <b xmlns:ndoc="urn:ndoc-schema">NumericField</b> disables norms and term freqs, as they are not usually needed during searching. If you need to change these settings, you should use this class. <p xmlns="urn:ndoc-schema"></p>See <b xmlns:ndoc="urn:ndoc-schema">NumericField</b> for capabilities of fields indexed numerically.<p xmlns="urn:ndoc-schema"></p><p xmlns="urn:ndoc-schema"></p>Here's an example usage, for an <code xmlns:ndoc="urn:ndoc-schema">int</code> field: <pre class="code" xmlns:ndoc="urn:ndoc-schema"> Field field = new Field(name, new NumericTokenStream(precisionStep).setIntValue(value));
 field.setOmitNorms(true);
 field.setOmitTermFreqAndPositions(true);
 document.add(field);
</pre><p xmlns="urn:ndoc-schema"></p>For optimal performance, re-use the TokenStream and Field instance for more than one document: <pre class="code" xmlns:ndoc="urn:ndoc-schema"> NumericTokenStream stream = new NumericTokenStream(precisionStep);
 Field field = new Field(name, stream);
 field.setOmitNorms(true);
 field.setOmitTermFreqAndPositions(true);
 Document document = new Document();
 document.add(field);

 for(all documents) {
   stream.setIntValue(value)
   writer.addDocument(document);
 }
</pre><p xmlns="urn:ndoc-schema"></p>This stream is not intended to be used in analyzers; it's more for iterating the different precisions during indexing a specific numeric value.<p xmlns="urn:ndoc-schema"></p><p xmlns="urn:ndoc-schema"></p><b xmlns="urn:ndoc-schema">NOTE</b>: as token streams are only consumed once the document is added to the index, if you index more than one numeric field, use a separate <code xmlns:ndoc="urn:ndoc-schema">NumericTokenStream</code> instance for each.<p xmlns="urn:ndoc-schema"></p><p xmlns="urn:ndoc-schema"></p>See <b xmlns:ndoc="urn:ndoc-schema">NumericRangeQuery(T)</b> for more details on the <a href="../search/NumericRangeQuery.html#precisionStepDesc" xmlns="urn:ndoc-schema"><code xmlns="" xmlns:ndoc="urn:ndoc-schema">precisionStep</code></a> parameter as well as how numeric fields work under the hood.<p xmlns="urn:ndoc-schema"></p><p xmlns="urn:ndoc-schema"></p><font color="red" xmlns="urn:ndoc-schema"><b>NOTE:</b> This API is experimental and might change in incompatible ways in the next release.</font> Since 2.9 </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.PerFieldAnalyzerWrapper.html">PerFieldAnalyzerWrapper</a></td><td width="50%"> This analyzer is used to facilitate scenarios where different fields require different analysis techniques. Use <a href="Lucene.Net.Analysis.PerFieldAnalyzerWrapper.AddAnalyzer.html">AddAnalyzer</a> to add a non-default analyzer on a field name basis. <p xmlns="urn:ndoc-schema"></p>Example usage: <pre class="code" xmlns:ndoc="urn:ndoc-schema">PerFieldAnalyzerWrapper aWrapper =
new PerFieldAnalyzerWrapper(new StandardAnalyzer());
aWrapper.addAnalyzer("firstname", new KeywordAnalyzer());
aWrapper.addAnalyzer("lastname", new KeywordAnalyzer());
</pre><p xmlns="urn:ndoc-schema"></p>In this example, StandardAnalyzer will be used for all fields except "firstname" and "lastname", for which KeywordAnalyzer will be used. <p xmlns="urn:ndoc-schema"></p>A PerFieldAnalyzerWrapper can be used like any other analyzer, for both indexing and query parsing. </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.PorterStemFilter.html">PorterStemFilter</a></td><td width="50%">Transforms the token stream as per the Porter stemming algorithm. Note: the input to the stemming filter must already be in lower case, so you will need to use LowerCaseFilter or LowerCaseTokenizer farther down the Tokenizer chain in order for this to work properly! <p xmlns="urn:ndoc-schema"></p> To use this filter with other analyzers, you'll want to write an Analyzer class that sets up the TokenStream chain as you want it. To use this with LowerCaseTokenizer, for example, you'd write an analyzer like this: <p xmlns="urn:ndoc-schema"></p><pre class="code" xmlns:ndoc="urn:ndoc-schema">class MyAnalyzer extends Analyzer {
    public final TokenStream tokenStream(String fieldName, Reader reader) {
         return new PorterStemFilter(new LowerCaseTokenizer(reader));
    }
}
</pre></td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.SimpleAnalyzer.html">SimpleAnalyzer</a></td><td width="50%">An <a href="Lucene.Net.Analysis.Analyzer.html">Analyzer</a> that filters <a href="Lucene.Net.Analysis.LetterTokenizer.html">LetterTokenizer</a> with <a href="Lucene.Net.Analysis.LowerCaseFilter.html">LowerCaseFilter</a></td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.StopAnalyzer.html">StopAnalyzer</a></td><td width="50%"> Filters <a href="Lucene.Net.Analysis.LetterTokenizer.html">LetterTokenizer</a> with <a href="Lucene.Net.Analysis.LowerCaseFilter.html">LowerCaseFilter</a> and <a href="Lucene.Net.Analysis.StopFilter.html">StopFilter</a>. <a name="version" xmlns="urn:ndoc-schema"></a><p xmlns="urn:ndoc-schema"></p> You must specify the required <a href="Lucene.Net.Util.Version.html">Version</a> compatibility when creating StopAnalyzer: <ul type="disc" xmlns:ndoc="urn:ndoc-schema"><li>As of 2.9, position increments are preserved</li></ul></td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.StopFilter.html">StopFilter</a></td><td width="50%"> Removes stop words from a token stream.</td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.TeeSinkTokenFilter.html">TeeSinkTokenFilter</a></td><td width="50%"> This TokenFilter provides the ability to set aside attribute states that have already been analyzed. This is useful in situations where multiple fields share many common analysis steps and then go their separate ways. <p xmlns="urn:ndoc-schema"></p> It is also useful for doing things like entity extraction or proper noun analysis as part of the analysis workflow and saving off those tokens for use in another field. <pre class="code" xmlns:ndoc="urn:ndoc-schema">TeeSinkTokenFilter source1 = new TeeSinkTokenFilter(new WhitespaceTokenizer(reader1));
TeeSinkTokenFilter.SinkTokenStream sink1 = source1.newSinkTokenStream();
TeeSinkTokenFilter.SinkTokenStream sink2 = source1.newSinkTokenStream();
TeeSinkTokenFilter source2 = new TeeSinkTokenFilter(new WhitespaceTokenizer(reader2));
source2.addSinkTokenStream(sink1);
source2.addSinkTokenStream(sink2);
TokenStream final1 = new LowerCaseFilter(source1);
TokenStream final2 = source2;
TokenStream final3 = new EntityDetect(sink1);
TokenStream final4 = new URLDetect(sink2);
d.add(new Field("f1", final1));
d.add(new Field("f2", final2));
d.add(new Field("f3", final3));
d.add(new Field("f4", final4));
</pre> In this example, <code xmlns:ndoc="urn:ndoc-schema">sink1</code> and <code xmlns:ndoc="urn:ndoc-schema">sink2</code> will both get tokens from both <code xmlns:ndoc="urn:ndoc-schema">reader1</code> and <code xmlns:ndoc="urn:ndoc-schema">reader2</code> after whitespace tokenizer and now we can further wrap any of these in extra analysis, and more "sources" can be inserted if desired. It is important, that tees are consumed before sinks (in the above example, the field names must be less the sink's field names). If you are not sure, which stream is consumed first, you can simply add another sink and then pass all tokens to the sinks at once using <a href="Lucene.Net.Analysis.TeeSinkTokenFilter.ConsumeAllTokens.html">ConsumeAllTokens</a>. This TokenFilter is exhausted after this. In the above example, change the example above to: <pre class="code" xmlns:ndoc="urn:ndoc-schema">...
TokenStream final1 = new LowerCaseFilter(source1.newSinkTokenStream());
TokenStream final2 = source2.newSinkTokenStream();
sink1.consumeAllTokens();
sink2.consumeAllTokens();
...
</pre> In this case, the fields can be added in any order, because the sources are not used anymore and all sinks are ready. <p xmlns="urn:ndoc-schema"></p>Note, the EntityDetect and URLDetect TokenStreams are for the example and do not currently exist in Lucene. </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.TeeSinkTokenFilter.AnonymousClassSinkFilter.html">TeeSinkTokenFilter.AnonymousClassSinkFilter</a></td><td width="50%"> </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.TeeSinkTokenFilter.SinkFilter.html">TeeSinkTokenFilter.SinkFilter</a></td><td width="50%"> A filter that decides which <a href="Lucene.Net.Util.AttributeSource.html">AttributeSource</a> states to store in the sink.</td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.TeeSinkTokenFilter.SinkTokenStream.html">TeeSinkTokenFilter.SinkTokenStream</a></td><td width="50%"> </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.Token.html">Token</a></td><td width="50%">A Token is an occurrence of a term from the text of a field. It consists of a term's text, the start and end offset of the term in the text of the field, and a type string. <p xmlns="urn:ndoc-schema"></p> The start and end offsets permit applications to re-associate a token with its source text, e.g., to display highlighted query terms in a document browser, or to show matching text fragments in a <abbr title="KeyWord In Context" xmlns="urn:ndoc-schema">KWIC</abbr> display, etc. <p xmlns="urn:ndoc-schema"></p> The type is a string, assigned by a lexical analyzer (a.k.a. tokenizer), naming the lexical or syntactic class that the token belongs to. For example an end of sentence marker token might be implemented with type "eos". The default token type is "word". <p xmlns="urn:ndoc-schema"></p> A Token can optionally have metadata (a.k.a. Payload) in the form of a variable length byte array. Use <a href="Lucene.Net.Index.TermPositions.PayloadLength.html">PayloadLength</a> and <a href="Lucene.Net.Index.TermPositions.GetPayload.html">GetPayload</a> to retrieve the payloads from the index. </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.Token.TokenAttributeFactory.html">Token.TokenAttributeFactory</a></td><td width="50%"><b xmlns="urn:ndoc-schema">Expert</b>: Creates an AttributeFactory returning {@link Token} as instance for the basic attributes and for all other attributes calls the given delegate factory. </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.TokenFilter.html">TokenFilter</a></td><td width="50%"> A TokenFilter is a TokenStream whose input is another TokenStream. <p xmlns="urn:ndoc-schema"></p> This is an abstract class; subclasses must override <a href="Lucene.Net.Analysis.TokenStream.IncrementToken.html">IncrementToken</a>. </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.Tokenizer.html">Tokenizer</a></td><td width="50%"> A Tokenizer is a TokenStream whose input is a Reader. <p xmlns="urn:ndoc-schema"></p> This is an abstract class; subclasses must override <a href="Lucene.Net.Analysis.TokenStream.IncrementToken.html">IncrementToken</a><p xmlns="urn:ndoc-schema"></p> NOTE: Subclasses overriding <b xmlns:ndoc="urn:ndoc-schema">IncrementToken</b> must call <a href="Lucene.Net.Util.AttributeSource.ClearAttributes.html">ClearAttributes</a> before setting attributes. </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.TokenStream.html">TokenStream</a></td><td width="50%"> A <code xmlns:ndoc="urn:ndoc-schema">TokenStream</code> enumerates the sequence of tokens, either from <a href="Lucene.Net.Documents.Field.html">Field</a>s of a <a href="Lucene.Net.Documents.Document.html">Document</a> or from query text. <p xmlns="urn:ndoc-schema"></p> This is an abstract class. Concrete subclasses are: <ul type="disc" xmlns:ndoc="urn:ndoc-schema"><li><a href="Lucene.Net.Analysis.Tokenizer.html">Tokenizer</a>, a <code>TokenStream</code> whose input is a Reader; and</li><li><a href="Lucene.Net.Analysis.TokenFilter.html">TokenFilter</a>, a <code>TokenStream</code> whose input is another <code>TokenStream</code>.</li></ul> A new <code xmlns:ndoc="urn:ndoc-schema">TokenStream</code> API has been introduced with Lucene 2.9. This API has moved from being <a href="Lucene.Net.Analysis.Token.html">Token</a> based to <a href="Lucene.Net.Util.IAttribute.html">IAttribute</a> based. While <b xmlns:ndoc="urn:ndoc-schema">Token</b> still exists in 2.9 as a convenience class, the preferred way to store the information of a <b xmlns:ndoc="urn:ndoc-schema">Token</b> is to use <a href="Lucene.Net.Util.Attribute.html">Attribute</a>s. <p xmlns="urn:ndoc-schema"></p><code xmlns:ndoc="urn:ndoc-schema">TokenStream</code> now extends <a href="Lucene.Net.Util.AttributeSource.html">AttributeSource</a>, which provides access to all of the token <b xmlns:ndoc="urn:ndoc-schema">IAttribute</b>s for the <code xmlns:ndoc="urn:ndoc-schema">TokenStream</code>. Note that only one instance per <b xmlns:ndoc="urn:ndoc-schema">Attribute</b> is created and reused for every token. This approach reduces object creation and allows local caching of references to the <b xmlns:ndoc="urn:ndoc-schema">Attribute</b>s. See <a href="Lucene.Net.Analysis.TokenStream.IncrementToken.html">IncrementToken</a> for further details. <p xmlns="urn:ndoc-schema"></p><b xmlns="urn:ndoc-schema">The workflow of the new <code xmlns="" xmlns:ndoc="urn:ndoc-schema">TokenStream</code> API is as follows:</b><ul type="disc" xmlns:ndoc="urn:ndoc-schema"><li>Instantiation of <code>TokenStream</code>/<b>TokenFilter</b>s which add/get attributes to/from the <b>AttributeSource</b>.</li><li>The consumer calls <a href="Lucene.Net.Analysis.TokenStream.Reset.html">Reset</a>.</li><li>The consumer retrieves attributes from the stream and stores local references to all attributes it wants to access</li><li>The consumer calls <b>IncrementToken</b> until it returns false and consumes the attributes after each call.</li><li>The consumer calls <a href="Lucene.Net.Analysis.TokenStream.End.html">End</a> so that any end-of-stream operations can be performed.</li><li>The consumer calls <a href="Lucene.Net.Analysis.TokenStream.Close.html">Close</a> to release any resource when finished using the <code>TokenStream</code></li></ul> To make sure that filters and consumers know which attributes are available, the attributes must be added during instantiation. Filters and consumers are not required to check for availability of attributes in <b xmlns:ndoc="urn:ndoc-schema">IncrementToken</b>. <p xmlns="urn:ndoc-schema"></p> You can find some example code for the new API in the analysis package level Javadoc. <p xmlns="urn:ndoc-schema"></p> Sometimes it is desirable to capture a current state of a <code xmlns:ndoc="urn:ndoc-schema">TokenStream</code> , e. g. for buffering purposes (see <a href="Lucene.Net.Analysis.CachingTokenFilter.html">CachingTokenFilter</a>, <a href="Lucene.Net.Analysis.TeeSinkTokenFilter.html">TeeSinkTokenFilter</a>). For this usecase <a href="Lucene.Net.Util.AttributeSource.CaptureState.html">CaptureState</a> and <a href="Lucene.Net.Util.AttributeSource.RestoreState.html">RestoreState</a> can be used. </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.WhitespaceAnalyzer.html">WhitespaceAnalyzer</a></td><td width="50%">An Analyzer that uses <a href="Lucene.Net.Analysis.WhitespaceTokenizer.html">WhitespaceTokenizer</a>. </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.WhitespaceTokenizer.html">WhitespaceTokenizer</a></td><td width="50%">A WhitespaceTokenizer is a tokenizer that divides text at whitespace. Adjacent sequences of non-Whitespace characters form tokens. </td></tr><tr valign="top"><td width="50%"><a href="Lucene.Net.Analysis.WordlistLoader.html">WordlistLoader</a></td><td width="50%"> Loader for text files that represent a list of stopwords.</td></tr></table></div></div></body></html>